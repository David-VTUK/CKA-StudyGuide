<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Part Five - Troubleshooting - CKA-Studyguide</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Part Five - Troubleshooting";
        var mkdocs_page_input_path = "revision-topics/05-troubleshooting.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> CKA-Studyguide
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Revision Topics</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../01-architcture-installation-configuration/">Part One - Architecture, Installation and Configuration</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../02-workloads-and-scheduling/">Part Two - Workloads & Scheduling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../03-services-and-networking/">Part Three - Services & Networking</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../04-storage/">Part Four - Storage</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Part Five - Troubleshooting</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#evaluate-cluster-and-node-logging">Evaluate cluster and node logging</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#master-nodes">Master Node(s)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#etcd">ETCD</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#kube-apiserver">Kube-apiserver</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#kube-scheduler">Kube-Scheduler</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#kube-controller-manager">Kube-Controller-Manager</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#worker-nodes">Worker Node(s)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cni">CNI</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#kube-proxy">Kube-Proxy</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#kubelet">Kubelet</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#container-runtime">Container Runtime</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cluster-logging">Cluster Logging</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#understand-how-to-monitor-applications">Understand how to monitor applications</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#manage-container-stdout-stderr-logs">Manage container stdout &amp; stderr logs</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#troubleshoot-application-failure">Troubleshoot application failure</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#troubleshoot-cluster-component-failure">Troubleshoot cluster component failure</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#troubleshoot-networking">Troubleshoot networking</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#dns-resolution">DNS Resolution</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cni-issues">CNI Issues</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#port-checking">Port Checking</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Lab Guide</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../lab-guide/00-general-advice/">Part Zero - General Advice</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../lab-guide/01-architcture-installation-configuration/">Part One - Architecture, Installation and Configuration</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../lab-guide/02-workloads-and-scheduling/">Part Two - Workloads & Scheduling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../lab-guide/03-services-and-networking/">Part Three - Services & Networking</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../lab-guide/04-storage/">Part Four - Storage</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../lab-guide/05-troubleshooting/">Part Five - Troubleshooting</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">CKA-Studyguide</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Revision Topics</li>
      <li class="breadcrumb-item active">Part Five - Troubleshooting</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="troubleshooting">Troubleshooting</h1>
<h2 id="evaluate-cluster-and-node-logging">Evaluate cluster and node logging</h2>
<h3 id="master-nodes">Master Node(s)</h3>
<h4 id="etcd">ETCD</h4>
<p>Usually, most etcd implementations also include etcdctl, which can aid in monitoring the state of the cluster. If you’re unsure where to find it, execute the following:</p>
<p><code>find / -name etcdctl</code></p>
<p>Leveraging this tool to check the cluster status:</p>
<pre class="highlight"><code class="language-bash">etcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status


+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|        ENDPOINT        |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://127.0.0.1:2379 | 4e30a295f2c3c1a4 |   3.5.0 |  8.1 MB |      true |      false |         3 |       7903 |               7903 |        |
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</code></pre>
<p>The cluster this was executed on has only one master node, hence only one result from the script. You will normally receive a response for each etcd member in the cluster.</p>
<p>Alternatively, leverage kubectl get componentstatuses:</p>
<pre class="highlight"><code class="language-bash">kubectl get componentstatuses #ComponentStatus is deprecated in v1.19+

NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-1               Healthy   {"health":"true"}    
etcd-0               Healthy   {"health":"true"} </code></pre>
<p>Etcd may also be running as a Pod:</p>
<pre class="highlight"><code class="language-shell">kubectl logs etcd-ubuntu -n kube-system</code></pre>
<h4 id="kube-apiserver">Kube-apiserver</h4>
<p>This is dependent on the environment for which the Kubernetes platform has been installed on. For systemd based systems:</p>
<pre class="highlight"><code class="language-bash">journalctl -u kube-apiserver</code></pre>
<p>Or</p>
<pre class="highlight"><code class="language-bash">cat /var/log/kube-apiserver.log</code></pre>
<p>Or for instances where Kube-API server is running as a static pod:</p>
<pre class="highlight"><code class="language-bash">kubectl logs kube-apiserver-k8s-master-03 -n kube-system</code></pre>
<h4 id="kube-scheduler">Kube-Scheduler</h4>
<p>For systemd-based systems</p>
<pre class="highlight"><code class="language-bash">journalctl -u kube-scheduler</code></pre>
<p>Or</p>
<pre class="highlight"><code class="language-bash">cat /var/log/kube-scheduler.log</code></pre>
<p>Or for instances where Kube-Scheduler is running as a static pod:</p>
<pre class="highlight"><code class="language-bash">kubectl logs kube-scheduler-k8s-master-03 -n kube-system</code></pre>
<h4 id="kube-controller-manager">Kube-Controller-Manager</h4>
<p>For systemd-based systems</p>
<pre class="highlight"><code class="language-bash">journalctl -u kube-controller-manager</code></pre>
<p>Or</p>
<pre class="highlight"><code class="language-bash">cat /var/log/kube-controller-manager.log</code></pre>
<p>Or for instances where Kube-controller manager is running as a static pod:</p>
<pre class="highlight"><code class="language-bash">kubectl logs kube-controller-manager-k8s-master-03 -n kube-system</code></pre>
<h3 id="worker-nodes">Worker Node(s)</h3>
<h4 id="cni">CNI</h4>
<p>Obviously this is dependent on the CNI in use for the cluster you’re working on. However, using Flannel as an example:</p>
<pre class="highlight"><code class="language-bash">journalctl -u flanneld</code></pre>
<p>If running as a pod, however:</p>
<pre class="highlight"><code class="language-shell">Kubectl logs --namespace kube-system &lt;POD-ID&gt; -c kube-flannel
kubectl logs --namespace kube-system weave-net-pwjkj -c weave</code></pre>
<h4 id="kube-proxy">Kube-Proxy</h4>
<p>For systemd-based systems</p>
<pre class="highlight"><code class="language-shell">journalctl -u kube-proxy</code></pre>
<p>Or</p>
<pre class="highlight"><code class="language-shell">cat /var/log/kube-proxy.log</code></pre>
<p>Or for instances where Kube-proxy manager is running as a static pod:</p>
<pre class="highlight"><code class="language-bash">kubectl logs kube-proxy -n kube-system</code></pre>
<h4 id="kubelet">Kubelet</h4>
<pre class="highlight"><code class="language-shell">journalctl -u kubelet</code></pre>
<p>Or</p>
<pre class="highlight"><code class="language-shell">cat /var/log/kubelet.log</code></pre>
<h4 id="container-runtime">Container Runtime</h4>
<p>Similarly to the CNI, this depends on which container runtime has been deployed, but using Docker as an example:</p>
<p>For systemd-based systems:</p>
<pre class="highlight"><code class="language-shell">journalctl -u docker.service</code></pre>
<p>Or</p>
<pre class="highlight"><code class="language-shell">cat /var/log/docker.log</code></pre>
<p>Hint : list the contents of <code>etc/systemd/system</code> if it’s a systemd-based service (containerd.service may be here)</p>
<h4 id="cluster-logging">Cluster Logging</h4>
<p>At a cluster level, <code>kubectl get events</code> provides a good overview.</p>
<h2 id="understand-how-to-monitor-applications">Understand how to monitor applications</h2>
<p>This section is a bit open-ended as it highly depends on what you have deployed and the topology of an application. Typically, however, we have an application that runs as a number of inter-connected <strong>microservices</strong>, consequently we monitor our applications by monitoring the underlying objects that comprise it, such as:</p>
<ul>
<li>Pods</li>
<li>Deployments</li>
<li>Services</li>
<li>etc</li>
</ul>
<h2 id="manage-container-stdout-stderr-logs">Manage container stdout &amp; stderr logs</h2>
<p><img alt="img.png" src="../images/logging.png" /></p>
<p>Kubernetes handles and redirects any output generated from a containers stdout and stderr streams. These get directed through a logging driver which influences where to store these logs. Different implementations of Docker differ in exact implementation (such as RHEL's flavor of Docker) but commonly, these drivers will write to a file in json format:</p>
<pre class="highlight"><code class="language-shell">root@ubuntu:~# docker info | grep "Logging Driver"
 Logging Driver: json-file</code></pre>
<p>The location for these logs is typically <code>/var/log/containers</code> but can be tweaked. Additionally, these contain symlinks:</p>
<pre class="highlight"><code class="language-shell">root@ubuntu:~# ls -la /var/log/containers/
total 44
drwxr-xr-x  2 root root   4096 Feb  8 19:18 .
drwxrwxr-x 11 root syslog 4096 Feb 12 00:00 ..
lrwxrwxrwx  1 root root    100 Feb  8 19:17 coredns-74ff55c5b-j4trd_kube-system_coredns-5d65324791ffcdf45d3552d875c6834f9a305c5be84b18745cb1657f784e5dd0.log -&gt; /var/log/pods/kube-system_coredns-74ff55c5b-j4trd_4afbef57-5592-4edb-96af-9d17f595d160/coredns/0.log
lrwxrwxrwx  1 root root    100 Feb  8 19:17 coredns-74ff55c5b-wrgkr_kube-system_coredns-b2fcfa679e9725dbe601bc1a0f218121a9c44b91d7300bbb57039a4edd219991.log -&gt; /var/log/pods/kube-system_coredns-74ff55c5b-wrgkr_b64ac6d5-654b-4194-b6a8-5f8aa4c3cbe2/coredns/0.log
lrwxrwxrwx  1 root root     81 Feb  8 19:16 etcd-ubuntu_kube-system_etcd-fcc5bc99932f380781776baa125b6f3be035e18fcec520afb827102e2afce1cd.log -&gt; /var/log/pods/kube-system_etcd-ubuntu_f608198a8b73b3cf090bd15e2823df04/etcd/0.log
lrwxrwxrwx  1 root root    101 Feb  8 19:16 kube-apiserver-ubuntu_kube-system_kube-apiserver-a264bbd54b7f23c8d424b0b368a48fdd1c5dcecc72fca95a460c146b2b5d85f5.log -&gt; /var/log/pods/kube-system_kube-apiserver-ubuntu_212641053a16fa2bb404ccde20f6eaf0/kube-apiserver/0.log
lrwxrwxrwx  1 root root    119 Feb  8 19:17 kube-controller-manager-ubuntu_kube-system_kube-controller-manager-a4ef7fe2b52272ea77f8de2da0989a9bcee757ae778fc08f1786b26b45bf13e1.log -&gt; /var/log/pods/kube-system_kube-controller-manager-ubuntu_7bbe7d37f1b2c7586237165580c2f5c3/kube-controller-manager/0.log
lrwxrwxrwx  1 root root    102 Feb  8 19:05 kube-flannel-ds-rfsfs_kube-system_install-cni-f8762e22fbf17925432682bdb1259a066208c62fa695d09cd6ee9b0cef3d36ba.log -&gt; /var/log/pods/kube-system_kube-flannel-ds-rfsfs_2892d4e3-e326-4b4b-90c0-396fb80863ca/install-cni/0.log
lrwxrwxrwx  1 root root    103 Feb  8 19:05 kube-flannel-ds-rfsfs_kube-system_kube-flannel-f96e019717814d7360e1aacd275cac121c13e0ee94cc5c93dcb35365608e6f83.log -&gt; /var/log/pods/kube-system_kube-flannel-ds-rfsfs_2892d4e3-e326-4b4b-90c0-396fb80863ca/kube-flannel/0.log
lrwxrwxrwx  1 root root     96 Feb  8 19:17 kube-proxy-l52f9_kube-system_kube-proxy-bfe08cb8663b46551e8608c094194ec61d03edfa7d25a6f414c07ed6563ada89.log -&gt; /var/log/pods/kube-system_kube-proxy-l52f9_d2b73ed1-5df4-4a18-9595-20798db4f110/kube-proxy/0.log
lrwxrwxrwx  1 root root    101 Feb  8 19:17 kube-scheduler-ubuntu_kube-system_kube-scheduler-4a695e53684f4591ec9385d6944f7841c0329aa49be220e5af6304da281cb41a.log -&gt; /var/log/pods/kube-system_kube-scheduler-ubuntu_69cd289b4ed80ced4f95a59ff60fa102/kube-scheduler/0.log</code></pre>
<h2 id="troubleshoot-application-failure">Troubleshoot application failure</h2>
<p>This is a somewhat ambitious topic to cover as how we approach troubleshooting application failures varies by the architecture of that application, which resources/API objects we're leveraging, if the application contains logs. However, good starting points would include running things like:</p>
<ul>
<li><code>kubectl describe &lt;object&gt;</code></li>
<li><code>kubectl logs &lt;podname&gt;</code></li>
<li><code>kubectl get events</code></li>
</ul>
<h2 id="troubleshoot-cluster-component-failure">Troubleshoot cluster component failure</h2>
<p>Covered in "Evaluate cluster and node logging"</p>
<h2 id="troubleshoot-networking">Troubleshoot networking</h2>
<h3 id="dns-resolution">DNS Resolution</h3>
<p><code>Pods</code> and <code>Services</code> will automatically have a DNS record registered against <code>coredns</code> in the cluster, aka "A" records for IPv4 and "AAAA" for IPv6. The format of which is:</p>
<p><code>pod-ip-address.my-namespace.pod.cluster-domain.example</code>
<code>my-svc-name.my-namespace.svc.cluster-domain.example</code></p>
<p>Pod DNS records resolve to a single entity, even if the Pod contains multiple containers as they share the same networking space.</p>
<p>Service DNS records resolve to the respective service object.</p>
<p>Pods will automatically have their DNS resolution configured based on coredns settings. This can be validated by opening a shell to the pod and inspecting /etc/resolv.conf:</p>
<pre class="highlight"><code class="language-shell">&gt; kubectl exec -it web-server sh
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
/ # cat /etc/resolv.conf 
nameserver 10.43.0.10
search default.svc.cluster.local svc.cluster.local cluster.local eu-central-1.compute.internal
options ndots:5</code></pre>
<p><code>10.43.0.10</code> being the coredns service object:</p>
<pre class="highlight"><code class="language-shell">&gt; kubectl get svc -n kube-system 
NAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE
kube-dns                     ClusterIP   10.43.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP         16d</code></pre>
<p>To test resolution, we can run a pod with <code>nslookup</code> to test. For the pod below:</p>
<pre class="highlight"><code class="language-shell">&gt; kubectl get po -o wide
NAME         READY   STATUS    RESTARTS   AGE     IP           NODE              NOMINATED NODE   READINESS GATES
web-server   1/1     Running   0          2d20h   10.42.1.31   ip-172-31-36-67   &lt;none&gt;           &lt;none&gt;</code></pre>
<p>Knowing the format of the A record:</p>
<p><code>pod-ip-address.my-namespace.pod.cluster-domain.example</code></p>
<p>We should be able to resolve <code>10-42-1-31.default.pod.cluster.local</code>. Tip : To determine the cluster domain, inspect the coredns configmap. Below indicating <code>cluster.local</code>.</p>
<pre class="highlight"><code class="language-shell">&gt; kubectl get cm coredns -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {</code></pre>
<p>Create a Pod with the tools required:</p>
<pre class="highlight"><code class="language-shell">kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml</code></pre>
<p>Test lookup:</p>
<pre class="highlight"><code class="language-shell">kubectl exec -i -t dnsutils -- nslookup 10-42-1-31.default.pod.cluster.local</code></pre>
<pre class="highlight"><code class="language-shell">&gt; kubectl exec -i -t dnsutils -- nslookup 10-42-1-31.default.pod.cluster.local
Server:         10.43.0.10
Address:        10.43.0.10#53

Name:   10-42-1-31.default.pod.cluster.local
Address: 10.42.1.31</code></pre>
<p>Similarly, for a service, in this case a service called <code>nginx-service</code> that resides in the default namespace:</p>
<pre class="highlight"><code class="language-shell">&gt; kubectl exec -i -t dnsutils -- nslookup nginx-service.default.svc.cluster.local
Server:         10.43.0.10
Address:        10.43.0.10#53

Name:   nginx-service.default.svc.cluster.local
Address: 10.43.0.223</code></pre>
<pre class="highlight"><code class="language-shell">&gt; kubectl get svc
NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
nginx-service   ClusterIP   10.43.0.223   &lt;none&gt;        80/TCP    9m15s</code></pre>
<h3 id="cni-issues">CNI Issues</h3>
<p>Mainly covered earlier in acquiring logs for the CNI. However, one issue that might occur is when a CNI is incorrectly, or not initialised. This may cause workloads to enter a <code>pending</code> status:</p>
<pre class="highlight"><code class="language-shell">kubectl get po -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nginx   0/1     Pending   0          57s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           &lt;none&gt;</code></pre>
<p><code>kubectl describe &lt;pod&gt;</code> can help identify issues with assigning IP addresses to nodes from the CNI</p>
<h3 id="port-checking">Port Checking</h3>
<p>Similarly, with leveraging <code>nslookup</code> to validate DNS resolution in our cluster, we can lean on other tools to perform other diagnostic. All we need is a pod that has a utility like <code>netcat</code>, <code>telnet</code> etc.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../04-storage/" class="btn btn-neutral float-left" title="Part Four - Storage"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../lab-guide/00-general-advice/" class="btn btn-neutral float-right" title="Part Zero - General Advice">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../04-storage/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../lab-guide/00-general-advice/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
